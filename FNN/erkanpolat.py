# -*- coding: utf-8 -*-
"""ErkanPolat

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YiydiP9_gYojN6SMSn4SBKpIQFUk-tm6
"""

from google.colab import drive
drive.mount('/content/drive')

"""LOAD DATA

"""

#Import the libraries
import zipfile
import os

zip_ref = zipfile.ZipFile('/content/drive/MyDrive/IMFDM.zip', 'r') #Opens the zip file in read mode
zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder
zip_ref.close()

len(os.listdir('/tmp/IMFDM/'))

"""

```
SIZE DIRECTORY
```

"""

import os

def get_dir_size_old(path='.'):
    total = 0
    for p in os.listdir(path):
        full_path = os.path.join(path, p)
        if os.path.isfile(full_path):
            total += os.path.getsize(full_path)
        elif os.path.isdir(full_path):
            total += get_dir_size_old(full_path)
    return total

print(get_dir_size_old('/tmp/IMFDM/'))
print(get_dir_size_old('/tmp/IMFDM/MIDDLE/'))
print(get_dir_size_old('/tmp/IMFDM/OLD/'))
print(get_dir_size_old('/tmp/IMFDM/YOUNG/'))

"""

```
IMPORT LIBRARIES
```

"""

# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

import os,cv2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

"""### TRAIN DATA"""

#Define Datapath
data_path = '/tmp/IMFDM/'
data_dir_list = os.listdir(data_path)

img_data_list=[]


for dataset in data_dir_list:
    img_list=os.listdir(data_path+'/'+ dataset)
    print ('Loaded the images of dataset-'+'{}\n'.format(dataset))
    for img in img_list:
        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )
        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)
        input_img_resize=cv2.resize(input_img,(32,32))
        img_data_list.append(input_img_resize)
        
img_data = np.array(img_data_list)
img_data = img_data.astype('float32')
img_data = img_data/255
img_data.shape

train_x = np.stack(img_data_list)
train_x.shape

datax = img_data[:, :, :, 0]
datax.shape

"""# LIBRARIES"""

# Commented out IPython magic to ensure Python compatibility.
import os,cv2
import shutil
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
 
from sklearn.model_selection import train_test_split
 

import keras
 
from keras import backend as K
 
from keras.models import Sequential, load_model
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Reshape, Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, UpSampling2D, LeakyReLU
from tensorflow.keras import layers
from tensorflow.keras import Model
 
# To handle image loading problem
from PIL import Image, ImageFile
 
# %matplotlib inline

"""# TEST DATA"""

#Import the libraries
import zipfile
import os

zip_ref = zipfile.ZipFile('/content/drive/MyDrive/Erkan.zip', 'r') #Opens the zip file in read mode
zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder
zip_ref.close()

len(os.listdir('/tmp/Erkan/'))

#Define Datapath
data_path1 = '/tmp/Erkan/'
data_dir_list = os.listdir(data_path1)


img_data_list1=[]


for dataset in data_dir_list:
    img_list=os.listdir(data_path1+'/'+ dataset)
    print ('Loaded the images of dataset-'+'{}\n'.format(dataset))
    for img in img_list:
        input_img=cv2.imread(data_path1 + '/'+ dataset + '/'+ img )
        #input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)
        input_img_resize=cv2.resize(input_img,(32,32))
        img_data_list1.append(input_img_resize)
        
img_data = np.array(img_data_list)
img_data = img_data.astype('float32')
img_data = img_data/255
img_data.shape

test_x = np.stack(img_data_list1)
test_x.shape

"""**Deneme**"""

train = pd.read_csv("/tmp/train.csv")
train.Class.value_counts()
test = pd.read_csv("/tmp/test.csv")

"""# **SHAPE**"""

class_names = ['YOUNG', 'OLD', 'MIDDLE']

num_classes = 3

from sklearn.preprocessing import LabelEncoder
import keras
lb = LabelEncoder()
train_y = lb.fit_transform(train.Class)
train_y = keras.utils.np_utils.to_categorical(train_y)
train.Class.value_counts(normalize=True)

print(train_x.shape)

print(test_x.shape)

"""*DELETE MODEL*"""

del model

"""# Q-2 MODEL

*50 epoch model*
"""

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

train_x.shape

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

"""# **CHANGES EPOCHS**"""

H = model.fit(train_x, train_y,epochs=5,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

H = model.fit(train_x, train_y,epochs=100,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

"""# **Q-6 MODEL**"""

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'adam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'SGD' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

"""# Q-4 MODEL"""

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(128, activation='relu'),
         tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(1024, activation='relu'),
         tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(1024, activation='relu'),
         tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

"""# Q-5 MODEL"""

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
          tf.keras.layers.Dense(512, activation='sigmoid'),

        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

"""# Q-3 MODEL"""

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
          tf.keras.layers.Dense(512, activation='tanh'),

        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
          tf.keras.layers.Dense(512, activation='selu'),

        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()

model = tf.keras.Sequential([

         tf.keras.layers.Flatten(input_shape = (32,32,3)),
         tf.keras.layers.Dense(512, activation='relu'),
          tf.keras.layers.Dense(512, activation='elu'),

        tf.keras.layers.Dense(3, activation='softmax')

         ])
 model.compile(optimizer = 'nadam' , loss='categorical_crossentropy', metrics=['accuracy'])

H = model.fit(train_x, train_y,epochs=50,verbose=1, validation_split=0.40)

# Commented out IPython magic to ensure Python compatibility.
# visualizing losses and accuracy
# %matplotlib inline

train_loss=H.history['loss']
val_loss=H.history['val_loss']
train_acc=H.history['accuracy']
val_acc=H.history['val_accuracy']

epochs = range(len(train_acc))

plt.plot(epochs,train_loss,'r', label='train_loss')
plt.plot(epochs,val_loss,'b', label='val_loss')
plt.title('train_loss vs val_loss')
plt.legend()
plt.figure()

plt.plot(epochs,train_acc,'r', label='train_acc')
plt.plot(epochs,val_acc,'b', label='val_acc')
plt.title('train_acc vs val_acc')
plt.legend()
plt.figure()